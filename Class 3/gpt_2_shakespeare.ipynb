{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source: \n",
        "\n",
        "*   https://pypi.org/project/gpt-2-simple/#description\n",
        "*   https://medium.com/@stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a\n",
        "*   https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=VHdTL8NDbAh3\n",
        "*  https://github.com/ak9250/gpt-2-colab\n",
        "*  https://www.aiweirdness.com/d-and-d-character-bios-now-making-19-03-15/\n",
        "*  https://minimaxir.com/2019/09/howto-gpt2/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rgNM-NcAZ9aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zawemi/GS2DIT/blob/main/Class%203/gpt_2_shakespeare.ipynb#scrollTo=4tIUvFbLMUuE)"
      ],
      "metadata": {
        "id": "4tIUvFbLMUuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's teach AI writing like a Shakespeare ðŸŽ“"
      ],
      "metadata": {
        "id": "MofLJqBHAWXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing the model"
      ],
      "metadata": {
        "id": "W7wiPFGQQn9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQACJ8lyUIR0",
        "outputId": "0cccac82-bba9-4197-8ea5-a2454f9cc20a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gpt-2-simple\n",
            "  Downloading gpt_2_simple-0.8.1.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (1.22.4)\n",
            "Collecting toposort\n",
            "  Downloading toposort-1.10-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.5.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.31.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.15.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.51.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (63.4.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.8.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.19.6)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (15.0.6.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (2.0.12)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.40.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.16.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.8.1-py3-none-any.whl size=24576 sha256=a3bd048e7931681e77775c2ce119f0d54a6ad7ee9743f164d3dfb32ada3b8414\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/28/f0/2f12e470be10d6804b193e4193d274c88995010fae512a67cf\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.8.1 toposort-1.10\n"
          ]
        }
      ],
      "source": [
        "#install the library we'll use today\n",
        "!pip install gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with basic model"
      ],
      "metadata": {
        "id": "ADzeFwzaQ8cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "d6Ah3D1CRK6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "mLg4pTPDaJJV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and let's download our AI model\n",
        "gpt2.download_gpt2()   # model is saved into current directory under /models/124M/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIXHjaxvaWsV",
        "outputId": "a37c21c1-f46a-4b6c-92d3-313c8e3afd9b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 516Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:02, 503kit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 470Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [01:13, 6.75Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 466Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:01, 844kit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:01, 844kit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "6CCkn75KbBpg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the model from file to use it\n",
        "gpt2.load_gpt2(sess, run_name='124M', checkpoint_dir='models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBvHQsxZsyP",
        "outputId": "67ed23c9-6eb6-4fb5-d9a5-52e4a49f7286"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "mDSFDj78RQJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how we would start model statement\n",
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "-P5_fxZOgGlk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is generating text\n",
        "gpt2.generate(sess, run_name='124M', checkpoint_dir='models', prefix=prefix, length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYqTat0gNDo",
        "outputId": "5ec79137-a2dc-4904-f1e1-29c19c0f7835"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth?\n",
            "\n",
            "Two Greenland ice sheets are being pulled apart by an enormous ice storm. Two different Greenland ice sheets are being pulled apart by an enormous ice storm.\n",
            "\n",
            "Earth was once a contender for a big planet, but now it's been rebelling\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with improved (finetuned) model"
      ],
      "metadata": {
        "id": "ML5helfmRjT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**\n",
        "</br>Restart the runtime (Runtime -> Restart runtime)"
      ],
      "metadata": {
        "id": "8cEaZKtRPx0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "NIPDKskeR7i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "eHys5-bWPnhJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get nietzsche texts\n",
        "!wget \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\""
      ],
      "metadata": {
        "id": "dRTQyR7IqaOl",
        "outputId": "1cedac2d-2559-4078-a8b6-79d833c525ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-22 12:46:42--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.144.157, 54.231.197.232, 52.216.51.32, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.144.157|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: â€˜nietzsche.txt.3â€™\n",
            "\n",
            "nietzsche.txt.3     100%[===================>] 586.82K   542KB/s    in 1.1s    \n",
            "\n",
            "2023-03-22 12:46:44 (542 KB/s) - â€˜nietzsche.txt.3â€™ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#game of thrones from https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books?select=001ssb.txt\n",
        "!gdown \"1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\"\n",
        "!mv /content/001ssb.txt /content/got1.txt"
      ],
      "metadata": {
        "id": "pzDNTjJzuKDW",
        "outputId": "cbb9ae69-c05b-45fd-9414-38435a957898",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\n",
            "To: /content/001ssb.txt\n",
            "\r  0% 0.00/1.63M [00:00<?, ?B/s]\r100% 1.63M/1.63M [00:00<00:00, 120MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's dowload a file with all Shakespeare plays\n",
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "!mv /content/input.txt /content/shakespeare.txt"
      ],
      "metadata": {
        "id": "9pwWGn5eqBJn",
        "outputId": "b59755af-4d8a-4592-c71a-bee603432b35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 15:19:13--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-03-21 15:19:13 (19.9 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "A0T2s8RxPnVr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teaching our model"
      ],
      "metadata": {
        "id": "bvllQvFxR9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetuning with shakespeare.txt (which, to be honest, means that we are teaching the model how to write like a shakespeare)\n",
        "#it takes a lot of time (~15min)...\n",
        "gpt2.finetune(sess, 'nietzsche.txt', steps=500)   # steps is max number of training steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJetxF6UOfY",
        "outputId": "73540521-bd43-46b7-d239-c80938c65c08"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 143770 tokens\n",
            "Training...\n",
            "[1 | 7.29] loss=4.18 avg=4.18\n",
            "[2 | 9.54] loss=3.95 avg=4.06\n",
            "[3 | 11.81] loss=3.94 avg=4.02\n",
            "[4 | 14.08] loss=3.89 avg=3.99\n",
            "[5 | 16.37] loss=3.74 avg=3.94\n",
            "[6 | 18.66] loss=3.90 avg=3.93\n",
            "[7 | 20.96] loss=3.85 avg=3.92\n",
            "[8 | 23.27] loss=3.76 avg=3.90\n",
            "[9 | 25.58] loss=3.75 avg=3.88\n",
            "[10 | 27.90] loss=3.67 avg=3.86\n",
            "[11 | 30.20] loss=3.82 avg=3.86\n",
            "[12 | 32.50] loss=3.78 avg=3.85\n",
            "[13 | 34.79] loss=3.81 avg=3.85\n",
            "[14 | 37.07] loss=3.63 avg=3.83\n",
            "[15 | 39.35] loss=3.71 avg=3.82\n",
            "[16 | 41.62] loss=3.50 avg=3.80\n",
            "[17 | 43.89] loss=3.56 avg=3.78\n",
            "[18 | 46.17] loss=3.53 avg=3.77\n",
            "[19 | 48.42] loss=3.59 avg=3.76\n",
            "[20 | 50.66] loss=3.56 avg=3.75\n",
            "[21 | 52.91] loss=3.60 avg=3.74\n",
            "[22 | 55.15] loss=3.56 avg=3.73\n",
            "[23 | 57.38] loss=3.52 avg=3.72\n",
            "[24 | 59.61] loss=3.56 avg=3.71\n",
            "[25 | 61.84] loss=3.46 avg=3.70\n",
            "[26 | 64.08] loss=3.48 avg=3.69\n",
            "[27 | 66.30] loss=3.38 avg=3.68\n",
            "[28 | 68.53] loss=3.40 avg=3.67\n",
            "[29 | 70.75] loss=3.41 avg=3.66\n",
            "[30 | 72.97] loss=3.61 avg=3.65\n",
            "[31 | 75.20] loss=3.31 avg=3.64\n",
            "[32 | 77.43] loss=3.45 avg=3.63\n",
            "[33 | 79.66] loss=3.36 avg=3.63\n",
            "[34 | 81.89] loss=3.50 avg=3.62\n",
            "[35 | 84.12] loss=3.61 avg=3.62\n",
            "[36 | 86.36] loss=3.14 avg=3.60\n",
            "[37 | 88.60] loss=3.35 avg=3.60\n",
            "[38 | 90.84] loss=3.37 avg=3.59\n",
            "[39 | 93.08] loss=3.47 avg=3.59\n",
            "[40 | 95.33] loss=3.33 avg=3.58\n",
            "[41 | 97.58] loss=3.44 avg=3.57\n",
            "[42 | 99.84] loss=3.27 avg=3.57\n",
            "[43 | 102.09] loss=3.51 avg=3.56\n",
            "[44 | 104.35] loss=3.44 avg=3.56\n",
            "[45 | 106.60] loss=3.52 avg=3.56\n",
            "[46 | 108.86] loss=3.33 avg=3.55\n",
            "[47 | 111.12] loss=3.21 avg=3.54\n",
            "[48 | 113.37] loss=3.36 avg=3.54\n",
            "[49 | 115.64] loss=3.53 avg=3.54\n",
            "[50 | 117.89] loss=3.41 avg=3.54\n",
            "[51 | 120.15] loss=3.53 avg=3.53\n",
            "[52 | 122.41] loss=3.19 avg=3.53\n",
            "[53 | 124.67] loss=3.33 avg=3.52\n",
            "[54 | 126.93] loss=3.34 avg=3.52\n",
            "[55 | 129.18] loss=3.36 avg=3.51\n",
            "[56 | 131.44] loss=3.53 avg=3.51\n",
            "[57 | 133.69] loss=3.27 avg=3.51\n",
            "[58 | 135.95] loss=3.34 avg=3.50\n",
            "[59 | 138.21] loss=3.35 avg=3.50\n",
            "[60 | 140.46] loss=3.04 avg=3.49\n",
            "[61 | 142.70] loss=3.06 avg=3.48\n",
            "[62 | 144.95] loss=3.19 avg=3.48\n",
            "[63 | 147.20] loss=3.28 avg=3.47\n",
            "[64 | 149.46] loss=3.33 avg=3.47\n",
            "[65 | 151.70] loss=3.23 avg=3.46\n",
            "[66 | 153.94] loss=3.11 avg=3.46\n",
            "[67 | 156.19] loss=3.42 avg=3.46\n",
            "[68 | 158.44] loss=2.98 avg=3.45\n",
            "[69 | 160.68] loss=3.18 avg=3.44\n",
            "[70 | 162.92] loss=2.94 avg=3.43\n",
            "[71 | 165.17] loss=3.20 avg=3.43\n",
            "[72 | 167.41] loss=3.23 avg=3.42\n",
            "[73 | 169.65] loss=3.21 avg=3.42\n",
            "[74 | 171.91] loss=3.07 avg=3.41\n",
            "[75 | 174.15] loss=3.03 avg=3.40\n",
            "[76 | 176.40] loss=3.03 avg=3.40\n",
            "[77 | 178.65] loss=3.12 avg=3.39\n",
            "[78 | 180.89] loss=3.38 avg=3.39\n",
            "[79 | 183.14] loss=3.03 avg=3.39\n",
            "[80 | 185.39] loss=3.29 avg=3.38\n",
            "[81 | 187.64] loss=2.95 avg=3.38\n",
            "[82 | 189.89] loss=3.04 avg=3.37\n",
            "[83 | 192.13] loss=2.99 avg=3.36\n",
            "[84 | 194.38] loss=3.10 avg=3.36\n",
            "[85 | 196.64] loss=3.13 avg=3.35\n",
            "[86 | 198.89] loss=3.35 avg=3.35\n",
            "[87 | 201.14] loss=3.09 avg=3.35\n",
            "[88 | 203.38] loss=3.11 avg=3.35\n",
            "[89 | 205.63] loss=3.27 avg=3.34\n",
            "[90 | 207.88] loss=3.05 avg=3.34\n",
            "[91 | 210.13] loss=2.90 avg=3.33\n",
            "[92 | 212.38] loss=3.24 avg=3.33\n",
            "[93 | 214.62] loss=2.95 avg=3.32\n",
            "[94 | 216.87] loss=3.09 avg=3.32\n",
            "[95 | 219.12] loss=2.80 avg=3.31\n",
            "[96 | 221.37] loss=3.22 avg=3.31\n",
            "[97 | 223.61] loss=3.26 avg=3.31\n",
            "[98 | 225.85] loss=3.09 avg=3.31\n",
            "[99 | 228.10] loss=3.04 avg=3.30\n",
            "[100 | 230.35] loss=3.09 avg=3.30\n",
            "======== SAMPLE 1 ========\n",
            " more, and consequently the more powerful, a power in whom power, in its opposite forms, and as a result, the most powerful form of form, is more difficult to attain. Even as the\n",
            "influence of a greater number, with whom one formerly\n",
            "had intercourse, has gradually brought the form back to itself. It never could\n",
            "feel any opposition, and its mind always became sensible of one thing or another.\n",
            "At the same time, when the number of intercourse-friendwaxes is much increasing\n",
            "and intercourse is taking its usual course, the general idea of a new\n",
            "friend, a personal one, develops, and the relation of the number of\n",
            "fellow-fellow intercourse-friends begins to become predominant! It will then,\n",
            "until now, still less frequently, there is one who is as old as friendship itself, and he\n",
            "wills all that is ancient, all that is new, all that is new in itself--in\n",
            "one man, in one society, all of humanity, everywhere--and it is not so difficult\n",
            "for a NEW man of his youth to find his way. But every one who, during this period,\n",
            "has had intercourse at his command, and who does not, in fact, have had\n",
            "relationship after intercourse, has only found himself as a type of\n",
            "\"friend,\" as some former comrades of the sort have formerly been--as people who\n",
            "have just as much in common as, in old times, have had in common! The type of the\n",
            "friend-friend, however, has hitherto been so much deeper and deeper than\n",
            "any one had hitherto imagined, that in the midst of all the new\n",
            "instincts (and indeed, in those ancient times, in the very depths and\n",
            "estimate of old fears!) it had to first appear upon the scene\n",
            "of all the new manifestations. It was a new type of man!--It was\n",
            "the kind, \"old fashioned,\" and \"new spirits,\" of the present day, but it was\n",
            "the old fashioned--and it was not, indeed, the type that brought about these\n",
            "dramatic changes in men; it was only the old fashioned type, and that\n",
            "was only the type which had changed so much, and the type\n",
            "which had gone AWOL. The \"old fashioned,\" as the friends of intercourse, made\n",
            "the appearance of some new type of man, the\n",
            "friend-friend, and he recognized himself as the type, in effect, the most\n",
            "old fashioned and the most old fashioned, and he praised intercourse for\n",
            "it bringing about this general and constant revolution in men, a sort of\n",
            "\"free spirit,\" and he praised intercourse for its being the \"peace of mind\" in\n",
            "man, as a sort of \"master hand,\" in man's hands. The friend-friend\n",
            "took back the old fashioned friendliness which had long stood\n",
            "thereby, the old fashioned friendship, which would even have been ashamed\n",
            "if man did not know it! To be sure, he still had to be careful and\n",
            "possess that kind of old fashioned friendliness which still had--\"the old\n",
            "fashioned\"--its little green and green-shrub-spiced eyes. At least the\n",
            "friendship of intercourse, the bonder of it, might not have been more\n",
            "old, more refined, more self-seeking, and more self-conceited! But the\n",
            "unfriendliness of the old fashioned, would not be much less friendlier! The\n",
            "unfriendliness of the new fashioned, and the new friendliness\n",
            "of the old fashioned--it is very easy now, very rare to understand\n",
            "it. One has only read in the last century of the science of\n",
            "relationship a philosophy of relationship--whereby it will be recognized that\n",
            "relationship--this kind of relationship of the type of which the\n",
            "philosopher of the world was alluding at the outset, the kind of\n",
            "relationship of the type of which the whole world was waiting to discover? At the\n",
            "right moment a man would say to himself who knew all he knew about relationships. The\n",
            "modern man cannot understand that which only old men (in the old age, however) recognized\n",
            "as truth: the kind of relationship of which relations are merely the evidence of\n",
            "the sort of thing the old was supposed to conceal\n",
            "and to have hidden away! What the ancient did not understand was that the\n",
            "modern man recognized relations among men as the evidence of something new\n",
            "which could be discovered. He recognized relations in their totality,\n",
            "and he saw the kinds of relations in those relations, together\n",
            "with some more refined, more self-reflective relations, relations of the\n",
            "kind, with which he regarded marriage as the least in his interest--it\n",
            "was something new in him--in which he could find no end. He\n",
            "never knew who he was: he was always quite hidden, never\n",
            "quite present; in his youth he was always quite absent; he\n",
            "\n",
            "\n",
            "[101 | 245.61] loss=2.78 avg=3.29\n",
            "[102 | 247.86] loss=3.05 avg=3.29\n",
            "[103 | 250.11] loss=2.89 avg=3.28\n",
            "[104 | 252.36] loss=2.75 avg=3.27\n",
            "[105 | 254.60] loss=3.18 avg=3.27\n",
            "[106 | 256.86] loss=3.16 avg=3.27\n",
            "[107 | 259.11] loss=2.91 avg=3.26\n",
            "[108 | 261.37] loss=3.03 avg=3.26\n",
            "[109 | 263.62] loss=2.85 avg=3.25\n",
            "[110 | 265.87] loss=2.78 avg=3.25\n",
            "[111 | 268.14] loss=2.70 avg=3.24\n",
            "[112 | 270.40] loss=2.95 avg=3.24\n",
            "[113 | 272.65] loss=2.66 avg=3.23\n",
            "[114 | 274.90] loss=2.84 avg=3.22\n",
            "[115 | 277.16] loss=2.85 avg=3.22\n",
            "[116 | 279.42] loss=2.93 avg=3.21\n",
            "[117 | 281.68] loss=2.70 avg=3.20\n",
            "[118 | 283.93] loss=3.04 avg=3.20\n",
            "[119 | 286.19] loss=2.55 avg=3.19\n",
            "[120 | 288.44] loss=2.69 avg=3.19\n",
            "[121 | 290.70] loss=2.75 avg=3.18\n",
            "[122 | 292.95] loss=2.86 avg=3.17\n",
            "[123 | 295.21] loss=2.92 avg=3.17\n",
            "[124 | 297.46] loss=2.74 avg=3.16\n",
            "[125 | 299.72] loss=2.90 avg=3.16\n",
            "[126 | 301.97] loss=2.85 avg=3.16\n",
            "[127 | 304.22] loss=2.58 avg=3.15\n",
            "[128 | 306.47] loss=2.87 avg=3.15\n",
            "[129 | 308.72] loss=2.72 avg=3.14\n",
            "[130 | 310.97] loss=2.85 avg=3.14\n",
            "[131 | 313.21] loss=2.58 avg=3.13\n",
            "[132 | 315.47] loss=2.64 avg=3.12\n",
            "[133 | 317.75] loss=2.73 avg=3.12\n",
            "[134 | 320.06] loss=2.78 avg=3.11\n",
            "[135 | 322.31] loss=2.74 avg=3.11\n",
            "[136 | 324.55] loss=2.61 avg=3.10\n",
            "[137 | 326.80] loss=2.69 avg=3.09\n",
            "[138 | 329.06] loss=2.53 avg=3.09\n",
            "[139 | 331.31] loss=2.39 avg=3.08\n",
            "[140 | 333.56] loss=2.19 avg=3.07\n",
            "[141 | 335.81] loss=2.64 avg=3.06\n",
            "[142 | 338.06] loss=2.48 avg=3.05\n",
            "[143 | 340.31] loss=2.59 avg=3.05\n",
            "[144 | 342.56] loss=2.48 avg=3.04\n",
            "[145 | 344.81] loss=2.49 avg=3.03\n",
            "[146 | 347.06] loss=2.34 avg=3.02\n",
            "[147 | 349.31] loss=2.64 avg=3.02\n",
            "[148 | 351.56] loss=2.62 avg=3.01\n",
            "[149 | 353.81] loss=2.74 avg=3.01\n",
            "[150 | 356.05] loss=2.45 avg=3.00\n",
            "[151 | 358.30] loss=2.51 avg=3.00\n",
            "[152 | 360.55] loss=2.54 avg=2.99\n",
            "[153 | 362.79] loss=2.42 avg=2.98\n",
            "[154 | 365.04] loss=2.57 avg=2.98\n",
            "[155 | 367.29] loss=2.55 avg=2.97\n",
            "[156 | 369.54] loss=2.39 avg=2.96\n",
            "[157 | 371.80] loss=2.18 avg=2.95\n",
            "[158 | 374.05] loss=2.35 avg=2.95\n",
            "[159 | 376.30] loss=2.33 avg=2.94\n",
            "[160 | 378.55] loss=2.29 avg=2.93\n",
            "[161 | 380.80] loss=2.28 avg=2.92\n",
            "[162 | 383.05] loss=2.43 avg=2.92\n",
            "[163 | 385.29] loss=2.18 avg=2.91\n",
            "[164 | 387.54] loss=2.47 avg=2.90\n",
            "[165 | 389.80] loss=2.68 avg=2.90\n",
            "[166 | 392.05] loss=2.45 avg=2.89\n",
            "[167 | 394.30] loss=2.64 avg=2.89\n",
            "[168 | 396.55] loss=2.75 avg=2.89\n",
            "[169 | 398.79] loss=2.24 avg=2.88\n",
            "[170 | 401.05] loss=2.26 avg=2.87\n",
            "[171 | 403.31] loss=2.43 avg=2.87\n",
            "[172 | 405.55] loss=2.39 avg=2.86\n",
            "[173 | 407.80] loss=2.08 avg=2.85\n",
            "[174 | 410.05] loss=1.91 avg=2.84\n",
            "[175 | 412.30] loss=2.20 avg=2.83\n",
            "[176 | 414.55] loss=2.04 avg=2.82\n",
            "[177 | 416.80] loss=1.94 avg=2.81\n",
            "[178 | 419.06] loss=2.02 avg=2.80\n",
            "[179 | 421.31] loss=2.16 avg=2.80\n",
            "[180 | 423.56] loss=2.66 avg=2.79\n",
            "[181 | 425.82] loss=2.19 avg=2.79\n",
            "[182 | 428.07] loss=2.16 avg=2.78\n",
            "[183 | 430.32] loss=2.25 avg=2.77\n",
            "[184 | 432.57] loss=2.10 avg=2.77\n",
            "[185 | 434.82] loss=2.40 avg=2.76\n",
            "[186 | 437.07] loss=2.10 avg=2.75\n",
            "[187 | 439.32] loss=2.20 avg=2.75\n",
            "[188 | 441.57] loss=1.90 avg=2.74\n",
            "[189 | 443.82] loss=1.94 avg=2.73\n",
            "[190 | 446.08] loss=2.11 avg=2.72\n",
            "[191 | 448.33] loss=2.40 avg=2.72\n",
            "[192 | 450.59] loss=2.21 avg=2.71\n",
            "[193 | 452.84] loss=2.12 avg=2.70\n",
            "[194 | 455.09] loss=2.23 avg=2.70\n",
            "[195 | 457.34] loss=2.19 avg=2.69\n",
            "[196 | 459.59] loss=2.01 avg=2.68\n",
            "[197 | 461.85] loss=1.94 avg=2.68\n",
            "[198 | 464.10] loss=1.92 avg=2.67\n",
            "[199 | 466.35] loss=3.22 avg=2.67\n",
            "[200 | 468.60] loss=1.69 avg=2.66\n",
            "======== SAMPLE 1 ========\n",
            " \"invented in order to have a heart as large as the palm of its hand\"--for the heart \"is not the greatest of [fundamental] forces, it is very delicate and cumbersome, and in any case it looks quite differently every time one wants it changed\" (p. 16).\n",
            "\n",
            "6. A principle which in the long run determines the conduct of civilization according to the \"nature\" of the \"favor\", and is henceforth called \"the utility principle\"\n",
            "(for \"nature\" at least means what is essentially operative and is a primary utility). A good\n",
            "\"universe\" or an arbitrary \"quantity of nothing\" determines the capacity of a\n",
            "badness to be\n",
            "hazarded. A moral is capable of being of this\n",
            "quantity in itself.\n",
            "\n",
            "7. Lack of a plurality leads to inability of others to\n",
            "attain any ultimate goal. Lack of any objective motive or cause manifests in\n",
            "as much as in its absence and with as great a moxie as the lack of the\n",
            "myth, but this is because the multifariousness of mankind does not permit it to\n",
            "attain its aim:--we are in the current infidels, the \"universalists\" and\n",
            "\"forgetfulness of things.\"--this has given rise to the famous remark that\n",
            "\"we are the ones who are the one, and one, and one; not the other\" (quoted in\n",
            "Man, Philosophy of Religion, vol. II, p. 54).\n",
            "\n",
            "8. That the individual man has an interest in his environment, for\n",
            "instance, does not appear from the foregoing to be new to the\n",
            "example of the collective actions and fauna in general, but is so\n",
            "externally, even lacking and lacking even in the general history of these\n",
            "individual acts of volition--that it has been hitherto unknown to\n",
            "man in what manner the environment of man evolved, how he evolved\n",
            "such an interest, in what relation he changed to others, or how he\n",
            "vital, sacrificial, charitable, voluntary, and utilitarian the\n",
            "environment has been: the entire historical conception, as given by\n",
            "Lambert, the collective action, the social organization, the collective\n",
            "judgment, the self-control, the tolerance, and also the freedom of the\n",
            "discernment; the whole \"man of the moment\"--man formerly stood here and\n",
            "acted out himself when he was young--is now regarded as something other than\n",
            "regard to today: something exceptional in its own right, and so on \"\n",
            "\"What? we do not act?\"--or, more simply, \"we are not what we are?\"\n",
            "The free spirit seems only a partial description--in fact freedom of the will\n",
            "seeks precisely this freedom. A bad man, however, who cannot get out of the way\n",
            "of his fellows, is regarded as something bad: so rigid and dictatorial, with a\n",
            "moral command, is the \"free spirit\",--a stern dwarf from whom a good man\n",
            "clings in only as an umbrella:--and a look of disgust is an indication that he\n",
            "is not very good-natured. It is precisely because of this superficiality, as\n",
            "certainly under the influence and influence of an emotion which prevail\n",
            "over human conduct, that many acts of defiance, some even betraying their\n",
            "conceitede-strand as a contempt, in short, as betraying something of the\n",
            "noble spirit (see above on the noble character of Diderot and Grenadiers).\n",
            "It is, however, very well notwithstanding, and perhaps because of the\n",
            "extent under which it has developed, that men can now speak honestly and\n",
            "towards other men, and not only against them, but also and even against\n",
            "their mothers, against their teachers, and equally and almost against themselves. A man who\n",
            "does not wish to be spoken of, and is not afraid to speak out in order to\n",
            "be loved, is thus forced to speak out: 'I want something feminine, I like\n",
            "it, but I can never find one willing to take it on my own terms, so I have to\n",
            "ask myself: what am I doing? And if this really means...' \"\n",
            "Well, it is a good question, and one that at this point in human history has\n",
            "caught the ear of the few who now call themselves skeptics, as that is\n",
            "their only true and only authorized name. One of the first of all is Arthur\n",
            "Gautrer, a German of German descent, who, like Arthur, was a skeptic and a\n",
            "badist, but who, as has been well known, had a look which made him\n",
            "regain happiness when he was on German soil, and is still admirably\n",
            "remembered.\n",
            "\n",
            "\n",
            "9. What kind of skeptic, what kind of a critic, is a skeptic who is a\n",
            "Skeptical who believes\n",
            "\n",
            "[201 | 481.94] loss=1.88 avg=2.65\n",
            "[202 | 484.18] loss=2.34 avg=2.65\n",
            "[203 | 486.44] loss=2.13 avg=2.64\n",
            "[204 | 488.69] loss=1.87 avg=2.63\n",
            "[205 | 490.94] loss=2.08 avg=2.63\n",
            "[206 | 493.19] loss=2.12 avg=2.62\n",
            "[207 | 495.44] loss=1.81 avg=2.61\n",
            "[208 | 497.69] loss=1.62 avg=2.60\n",
            "[209 | 499.93] loss=1.76 avg=2.59\n",
            "[210 | 502.18] loss=1.91 avg=2.58\n",
            "[211 | 504.44] loss=1.53 avg=2.57\n",
            "[212 | 506.69] loss=2.28 avg=2.57\n",
            "[213 | 508.94] loss=2.28 avg=2.57\n",
            "[214 | 511.19] loss=2.27 avg=2.56\n",
            "[215 | 513.44] loss=2.08 avg=2.56\n",
            "[216 | 515.68] loss=2.00 avg=2.55\n",
            "[217 | 517.93] loss=1.93 avg=2.54\n",
            "[218 | 520.18] loss=1.84 avg=2.54\n",
            "[219 | 522.43] loss=2.28 avg=2.53\n",
            "[220 | 524.68] loss=2.17 avg=2.53\n",
            "[221 | 526.93] loss=2.21 avg=2.53\n",
            "[222 | 529.18] loss=1.71 avg=2.52\n",
            "[223 | 531.43] loss=1.85 avg=2.51\n",
            "[224 | 533.69] loss=2.01 avg=2.50\n",
            "[225 | 535.94] loss=1.44 avg=2.49\n",
            "[226 | 538.19] loss=1.72 avg=2.48\n",
            "[227 | 540.44] loss=1.85 avg=2.48\n",
            "[228 | 542.69] loss=1.83 avg=2.47\n",
            "[229 | 544.94] loss=1.58 avg=2.46\n",
            "[230 | 547.19] loss=2.04 avg=2.45\n",
            "[231 | 549.43] loss=1.78 avg=2.45\n",
            "[232 | 551.68] loss=1.25 avg=2.43\n",
            "[233 | 553.93] loss=1.83 avg=2.43\n",
            "[234 | 556.18] loss=1.37 avg=2.42\n",
            "[235 | 558.43] loss=1.45 avg=2.40\n",
            "[236 | 560.68] loss=1.88 avg=2.40\n",
            "[237 | 562.93] loss=2.04 avg=2.39\n",
            "[238 | 565.17] loss=1.66 avg=2.39\n",
            "[239 | 567.42] loss=2.10 avg=2.38\n",
            "[240 | 569.68] loss=1.70 avg=2.38\n",
            "[241 | 571.94] loss=1.44 avg=2.37\n",
            "[242 | 574.19] loss=1.29 avg=2.35\n",
            "[243 | 576.44] loss=1.72 avg=2.35\n",
            "[244 | 578.69] loss=1.25 avg=2.33\n",
            "[245 | 580.94] loss=1.73 avg=2.33\n",
            "[246 | 583.19] loss=2.14 avg=2.33\n",
            "[247 | 585.44] loss=1.26 avg=2.31\n",
            "[248 | 587.68] loss=1.53 avg=2.31\n",
            "[249 | 589.93] loss=1.54 avg=2.30\n",
            "[250 | 592.18] loss=1.34 avg=2.29\n",
            "[251 | 594.43] loss=1.61 avg=2.28\n",
            "[252 | 596.68] loss=1.86 avg=2.28\n",
            "[253 | 598.93] loss=1.49 avg=2.27\n",
            "[254 | 601.18] loss=1.72 avg=2.26\n",
            "[255 | 603.43] loss=1.54 avg=2.25\n",
            "[256 | 605.68] loss=1.54 avg=2.25\n",
            "[257 | 607.92] loss=1.48 avg=2.24\n",
            "[258 | 610.17] loss=1.39 avg=2.23\n",
            "[259 | 612.42] loss=1.29 avg=2.22\n",
            "[260 | 614.67] loss=1.09 avg=2.21\n",
            "[261 | 616.91] loss=1.57 avg=2.20\n",
            "[262 | 619.17] loss=1.38 avg=2.19\n",
            "[263 | 621.41] loss=1.81 avg=2.19\n",
            "[264 | 623.66] loss=1.13 avg=2.17\n",
            "[265 | 625.91] loss=1.43 avg=2.17\n",
            "[266 | 628.16] loss=1.52 avg=2.16\n",
            "[267 | 630.41] loss=1.59 avg=2.15\n",
            "[268 | 632.65] loss=1.42 avg=2.15\n",
            "[269 | 634.90] loss=1.30 avg=2.14\n",
            "[270 | 637.15] loss=1.30 avg=2.13\n",
            "[271 | 639.40] loss=1.22 avg=2.12\n",
            "[272 | 641.66] loss=1.13 avg=2.11\n",
            "[273 | 643.91] loss=1.16 avg=2.10\n",
            "[274 | 646.17] loss=1.70 avg=2.09\n",
            "[275 | 648.43] loss=1.04 avg=2.08\n",
            "[276 | 650.68] loss=1.16 avg=2.07\n",
            "[277 | 652.94] loss=1.33 avg=2.06\n",
            "[278 | 655.21] loss=1.09 avg=2.05\n",
            "[279 | 657.46] loss=0.93 avg=2.04\n",
            "[280 | 659.72] loss=1.13 avg=2.03\n",
            "[281 | 661.97] loss=1.03 avg=2.02\n",
            "[282 | 664.24] loss=1.19 avg=2.01\n",
            "[283 | 666.49] loss=1.45 avg=2.01\n",
            "[284 | 668.75] loss=1.00 avg=2.00\n",
            "[285 | 671.01] loss=1.16 avg=1.99\n",
            "[286 | 673.27] loss=0.92 avg=1.98\n",
            "[287 | 675.53] loss=1.18 avg=1.97\n",
            "[288 | 677.78] loss=1.53 avg=1.96\n",
            "[289 | 680.04] loss=1.02 avg=1.95\n",
            "[290 | 682.29] loss=1.09 avg=1.94\n",
            "[291 | 684.53] loss=1.04 avg=1.93\n",
            "[292 | 686.78] loss=1.01 avg=1.92\n",
            "[293 | 689.03] loss=0.87 avg=1.91\n",
            "[294 | 691.29] loss=0.96 avg=1.90\n",
            "[295 | 693.54] loss=1.47 avg=1.90\n",
            "[296 | 695.79] loss=1.13 avg=1.89\n",
            "[297 | 698.04] loss=1.08 avg=1.88\n",
            "[298 | 700.30] loss=0.89 avg=1.87\n",
            "[299 | 702.55] loss=1.16 avg=1.86\n",
            "[300 | 704.81] loss=1.20 avg=1.86\n",
            "======== SAMPLE 1 ========\n",
            "theistic. The question then becomes,\"\n",
            "\n",
            "                                 14.\n",
            "\n",
            "                            My heart wishes,\n",
            "\n",
            "                          15.\n",
            "\n",
            "                        My hope seeks,\n",
            "\n",
            "                       My hope wants,\n",
            "\n",
            "                    My hope rambered.\n",
            "\n",
            "                      My hope laid hold of\n",
            "Her: and the angel of death and salvation looked up.\n",
            "\n",
            "                      My goodness was she so\n",
            "eager and furious, so resolute and enduring? Had not she gone much too low?\n",
            "How did she get so mad? What was she truly mader then? She\n",
            "elevated herself above the many, enthroned themselves upon earth. She\n",
            "met with them once more: \"Etre sombre odes et propter nia me jamais.\"\n",
            "\n",
            "                           My peace and truth\n",
            "are infinite, it                     My salvation is\n",
            "                        My faith in the\n",
            "miraculous, in the miracles were in heaven: \"It is written in the craggy\n",
            "of Star Tours: 'There is a thorn that lives to this day, and a hundred\n",
            "thousands that have not yet seen it.'\" This queen of fate was not\n",
            "willing to be missed or disrespected. She stood before the one and only\n",
            "real god, the god of justice and justice in the world, whose hand\n",
            "was fastening away wrath: she was, perhaps, more beautiful than\n",
            "that of Caesar. As such, however, and bearing equally the signature\n",
            "of TRUST and FALSTRUCTURE (in Greek domed pia natura), the latter was a\n",
            "relic of the most learned of all mediators, who had been brought to\n",
            "this, that and such proceedings, with a proper reverence and seriousness,\n",
            "their agreement and understanding, are the touchstones of the history of\n",
            "all knowledge--the work that has now been put into practice for the first\n",
            "century A.D..--Through an almost unbroken succession of sudden\n",
            "powerful religious forces, the history of the arts and science has\n",
            "developed itself firmly and irresistibly to the basic questions: Is\n",
            "instinct intentional or not?--How does one know something is true or false? Through\n",
            "a mixture of bad and good feelings; through an encounter and contrast of\n",
            "desire and knowledge; through a distrust of reason and its consequences;\n",
            "through, in a sense, up to that time, the frightful certainty that things\n",
            "were--different. In short, the history of knowledge is the history of\n",
            "the free exchange of wishes and experiences for absolute truths, or, to be\n",
            "more exact, of knowledge as a whole. Illogical people would like\n",
            "to believe that whatever is, absolutely must be; illogical people would\n",
            "like to view things in their totality, and generally a bad\n",
            "explanation is involved in the explanation (the \"dullieu\"). The free\n",
            "evil, therefore, is not only all people of knowledge, but necessarily\n",
            "every one knows what he is doing is illogical.\n",
            "\n",
            "\n",
            "58\n",
            "\n",
            "=Sympathy and Mutual Supplication.=--During a long life, individuals have\n",
            "welfare and distinguish right and wrong in connection with each other. We may\n",
            "believe that nature must cause injury to man in order that man may become\n",
            "stable and in need of nature's intervention, so that man may grow healthy and\n",
            "healthy in nature.--Hence man is illogical because he resembles neither\n",
            "nor comes close to understanding. Thus arises the terrible perplexity:\n",
            "\"What is the matter?\"\"What is the nature of the matter?\"\"Why do things\n",
            "this way and so?\" The individuals thus get the answer: the\n",
            "nature of the matter.\"--This is the terrible pia nard in which all\n",
            "theory has been trained. Aye, but without the knowledge necessary\n",
            "\n",
            "\n",
            "[301 | 718.06] loss=0.97 avg=1.85\n",
            "[302 | 720.31] loss=0.55 avg=1.83\n",
            "[303 | 722.56] loss=1.24 avg=1.83\n",
            "[304 | 724.80] loss=1.12 avg=1.82\n",
            "[305 | 727.05] loss=0.79 avg=1.81\n",
            "[306 | 729.31] loss=0.74 avg=1.80\n",
            "[307 | 731.57] loss=1.07 avg=1.79\n",
            "[308 | 733.82] loss=1.09 avg=1.78\n",
            "[309 | 736.06] loss=0.96 avg=1.77\n",
            "[310 | 738.32] loss=0.95 avg=1.77\n",
            "[311 | 740.57] loss=0.85 avg=1.76\n",
            "[312 | 742.82] loss=1.30 avg=1.75\n",
            "[313 | 745.07] loss=0.61 avg=1.74\n",
            "[314 | 747.32] loss=1.02 avg=1.73\n",
            "[315 | 749.57] loss=0.63 avg=1.72\n",
            "[316 | 751.82] loss=0.98 avg=1.71\n",
            "[317 | 754.07] loss=1.15 avg=1.71\n",
            "[318 | 756.31] loss=1.18 avg=1.70\n",
            "[319 | 758.56] loss=0.74 avg=1.69\n",
            "[320 | 760.81] loss=1.06 avg=1.68\n",
            "[321 | 763.07] loss=1.28 avg=1.68\n",
            "[322 | 765.32] loss=1.13 avg=1.67\n",
            "[323 | 767.57] loss=0.63 avg=1.66\n",
            "[324 | 769.82] loss=0.56 avg=1.65\n",
            "[325 | 772.06] loss=0.82 avg=1.64\n",
            "[326 | 774.32] loss=0.92 avg=1.64\n",
            "[327 | 776.57] loss=0.52 avg=1.62\n",
            "[328 | 778.82] loss=0.70 avg=1.61\n",
            "[329 | 781.06] loss=1.23 avg=1.61\n",
            "[330 | 783.32] loss=0.57 avg=1.60\n",
            "[331 | 785.56] loss=0.75 avg=1.59\n",
            "[332 | 787.82] loss=1.04 avg=1.59\n",
            "[333 | 790.06] loss=0.69 avg=1.58\n",
            "[334 | 792.32] loss=0.67 avg=1.57\n",
            "[335 | 794.57] loss=0.80 avg=1.56\n",
            "[336 | 796.81] loss=0.62 avg=1.55\n",
            "[337 | 799.07] loss=1.03 avg=1.54\n",
            "[338 | 801.31] loss=0.53 avg=1.53\n",
            "[339 | 803.56] loss=0.41 avg=1.52\n",
            "[340 | 805.81] loss=0.69 avg=1.51\n",
            "[341 | 808.06] loss=0.70 avg=1.50\n",
            "[342 | 810.30] loss=0.71 avg=1.50\n",
            "[343 | 812.56] loss=0.69 avg=1.49\n",
            "[344 | 814.81] loss=0.75 avg=1.48\n",
            "[345 | 817.06] loss=0.55 avg=1.47\n",
            "[346 | 819.31] loss=0.58 avg=1.46\n",
            "[347 | 821.56] loss=0.58 avg=1.45\n",
            "[348 | 823.82] loss=0.50 avg=1.44\n",
            "[349 | 826.07] loss=0.81 avg=1.44\n",
            "[350 | 828.32] loss=0.53 avg=1.43\n",
            "[351 | 830.57] loss=0.33 avg=1.42\n",
            "[352 | 832.82] loss=0.75 avg=1.41\n",
            "[353 | 835.07] loss=0.60 avg=1.40\n",
            "[354 | 837.32] loss=0.48 avg=1.39\n",
            "[355 | 839.57] loss=0.85 avg=1.39\n",
            "[356 | 841.82] loss=0.73 avg=1.38\n",
            "[357 | 844.07] loss=0.63 avg=1.37\n",
            "[358 | 846.33] loss=0.67 avg=1.36\n",
            "[359 | 848.58] loss=0.76 avg=1.36\n",
            "[360 | 850.82] loss=0.63 avg=1.35\n",
            "[361 | 853.06] loss=0.74 avg=1.34\n",
            "[362 | 855.31] loss=0.35 avg=1.33\n",
            "[363 | 857.56] loss=0.80 avg=1.33\n",
            "[364 | 859.80] loss=0.57 avg=1.32\n",
            "[365 | 862.05] loss=0.45 avg=1.31\n",
            "[366 | 864.30] loss=0.54 avg=1.30\n",
            "[367 | 866.54] loss=0.60 avg=1.30\n",
            "[368 | 868.79] loss=0.49 avg=1.29\n",
            "[369 | 871.04] loss=0.38 avg=1.28\n",
            "[370 | 873.29] loss=0.68 avg=1.27\n",
            "[371 | 875.54] loss=0.51 avg=1.27\n",
            "[372 | 877.78] loss=0.26 avg=1.25\n",
            "[373 | 880.03] loss=0.48 avg=1.25\n",
            "[374 | 882.28] loss=0.34 avg=1.24\n",
            "[375 | 884.54] loss=0.84 avg=1.23\n",
            "[376 | 886.79] loss=0.55 avg=1.23\n",
            "[377 | 889.05] loss=0.49 avg=1.22\n",
            "[378 | 891.30] loss=0.35 avg=1.21\n",
            "[379 | 893.55] loss=0.45 avg=1.20\n",
            "[380 | 895.82] loss=0.39 avg=1.19\n",
            "[381 | 898.07] loss=0.43 avg=1.19\n",
            "[382 | 900.33] loss=0.61 avg=1.18\n",
            "[383 | 902.58] loss=0.38 avg=1.17\n",
            "[384 | 904.83] loss=0.49 avg=1.17\n",
            "[385 | 907.10] loss=0.56 avg=1.16\n",
            "[386 | 909.35] loss=0.49 avg=1.15\n",
            "[387 | 911.60] loss=0.59 avg=1.15\n",
            "[388 | 913.86] loss=0.39 avg=1.14\n",
            "[389 | 916.11] loss=0.60 avg=1.13\n",
            "[390 | 918.36] loss=0.67 avg=1.13\n",
            "[391 | 920.61] loss=0.36 avg=1.12\n",
            "[392 | 922.86] loss=0.29 avg=1.11\n",
            "[393 | 925.11] loss=0.60 avg=1.11\n",
            "[394 | 927.35] loss=0.43 avg=1.10\n",
            "[395 | 929.60] loss=0.52 avg=1.09\n",
            "[396 | 931.86] loss=0.56 avg=1.09\n",
            "[397 | 934.11] loss=0.45 avg=1.08\n",
            "[398 | 936.36] loss=0.46 avg=1.08\n",
            "[399 | 938.61] loss=0.38 avg=1.07\n",
            "[400 | 940.86] loss=0.44 avg=1.06\n",
            "======== SAMPLE 1 ========\n",
            " and to go forward into the\n",
            "forestry of their dreams. There is far too much cold blooded \"intoxication\" in\n",
            "the matter.\n",
            "\n",
            "28. There is an immense unspoiled chaos of discontent and misunderstanding,\n",
            "embracing and retiring back to the foundation, as from a hot spring.\n",
            "The naive and the delightable in this condition, have hitherto shouted\n",
            "too loudly: that which is at present useful, may be\n",
            "liable in the near future! But what is this \"short-sightedness,\"\n",
            "which is frequently thrown away at the stake, in the reasonings of\n",
            "the affairs of states, in the consensus of the states? \"It\n",
            "IS US, it is \"just the weather,\" \"it happens so often\"--who knows\n",
            "what such nonsense! WHAT is this \"Floor-Orderness,\" and all that\n",
            "delicate \"Intoxication,\" to borrow the rhetorician's phrase,\n",
            "from the physicians' box? Oh, doctors, what strange and over-the-top\n",
            "horrors! And with regard to our common enemies--such as such \"Floor-Order\n",
            "Hooks,\" there is perhaps a better name for them (primum fortified\n",
            "suspicionis, in pity and credit to God!)--they appear on the\n",
            "whole, as hallucinations and masquerades (and transfigurations of\n",
            "many kinds, as from another world) to frighten, petrify,\n",
            "bother, terrorize. There is so much \"misunderstanding\" about\n",
            "it that Europe has become one with it (Erscheidenung, suicide\n",
            "sentiment stellung, to translate Luther's De Nocio Sanctae). There is\n",
            "too much \"Bible interpretation\" and \"Catholic theological\n",
            "interpretation\" (by means of Luther's son) such as has been gained through\n",
            "the education of such \"Whites\" (Schopenhauer)--and there may be still\n",
            "preliminary grounds for supposing that such \"Nouns\" constitute the\n",
            "origin of man, have gained the ascendancy over the unintelligible,\n",
            "as a kind of music-divining stone that has won admiration and\n",
            "doubts among scholars and discoverers (GrÃ¼ndigkeit,\n",
            "\"Der Naturwesenschaft, literally, Intelligence, Poverty,\n",
            "Power, Darkness, and Change, was the beauty of it: and if the\n",
            "universality which is in too great a city or state can never acquire\n",
            "any sufficiently mature Natures, that under absolutely absolutely dangerous\n",
            "moralities, can also give expression to the Morphisms). Among\n",
            "animal peoples, on the other hand, no race has attained such\n",
            "genius at the basis of such understanding[14] as the\n",
            "Flat Continent calls to mind and deserves to be praised:--the Europeans\n",
            "do not seem to think this to be possible, because on the whole there\n",
            "is not a wholly voluntary subversion of man's nature, but rather voluntary\n",
            "sublimity-seeking, sublimity-seeking for man. The species in question wants only\n",
            "mutual salvation, the same essentially UNIFICATED as on earth, but\n",
            "not in the least influenced by any enticement or holy book ascribed\n",
            "to it. The Europeans do not feel a priori any obligation whatever\n",
            "to RECOGNISE A LEVELLER, any great duty whatever, except that it\n",
            "might \"change the course of the universe\"? But such \"mutual\n",
            "solicitations\" do NOT revolve around man, their root cause being the\n",
            "super-abundance of authority in Europe. The Europeans feel no\n",
            "duty whatever in relation to man, except as \"labour\" and\n",
            "\"providing.\" There is a SUPER-ARCHOTHER METHOD OF RELIGION IN THEM... One\n",
            "could say to them: \"You like man so much, you want to make him\n",
            "so much better, you want to make him learn to love man, and if possible,\n",
            "SLAVE HIM TO LIE. In this light, they send out so-called \"willing\n",
            "hearted\" to do just that; but what difference does it make if the latter\n",
            "\"DOLLARS\" (the noble auxiliaries)--! do so in the presence of a\n",
            "\"Willing\"! In the name of \"higher learning,\" a species of\n",
            "\"mankind,\" a species of genius!\n",
            "\n",
            "[14] That made an introduction and drew near,\n",
            "those Europeans who have been slain by\n",
            "an oncoming German on foot, be it by their Will to Lead,\n",
            "\"L'AUTHORIER DE FANTASTIE\" (their parlance) or by the echo of\n",
            "their own bell-ring,--be it in the barbed-edged chime or the mutinous\n",
            "glorification cry of their Aeon in the grove, their own language\n",
            "or\n",
            "\n",
            "[401 | 954.03] loss=0.44 avg=1.06\n",
            "[402 | 956.28] loss=0.55 avg=1.05\n",
            "[403 | 958.53] loss=0.36 avg=1.04\n",
            "[404 | 960.78] loss=0.42 avg=1.04\n",
            "[405 | 963.02] loss=0.51 avg=1.03\n",
            "[406 | 965.27] loss=0.40 avg=1.03\n",
            "[407 | 967.52] loss=0.62 avg=1.02\n",
            "[408 | 969.77] loss=0.38 avg=1.01\n",
            "[409 | 972.02] loss=0.54 avg=1.01\n",
            "[410 | 974.27] loss=0.41 avg=1.00\n",
            "[411 | 976.52] loss=0.52 avg=1.00\n",
            "[412 | 978.77] loss=0.37 avg=0.99\n",
            "[413 | 981.02] loss=0.55 avg=0.99\n",
            "[414 | 983.27] loss=0.50 avg=0.98\n",
            "[415 | 985.52] loss=0.26 avg=0.98\n",
            "[416 | 987.77] loss=0.26 avg=0.97\n",
            "[417 | 990.01] loss=0.67 avg=0.97\n",
            "[418 | 992.26] loss=0.27 avg=0.96\n",
            "[419 | 994.51] loss=0.42 avg=0.95\n",
            "[420 | 996.75] loss=0.42 avg=0.95\n",
            "[421 | 999.00] loss=0.30 avg=0.94\n",
            "[422 | 1001.25] loss=0.32 avg=0.93\n",
            "[423 | 1003.50] loss=0.28 avg=0.93\n",
            "[424 | 1005.75] loss=0.34 avg=0.92\n",
            "[425 | 1007.99] loss=0.22 avg=0.91\n",
            "[426 | 1010.24] loss=0.27 avg=0.91\n",
            "[427 | 1012.49] loss=0.24 avg=0.90\n",
            "[428 | 1014.74] loss=0.27 avg=0.90\n",
            "[429 | 1017.00] loss=0.34 avg=0.89\n",
            "[430 | 1019.25] loss=0.29 avg=0.88\n",
            "[431 | 1021.50] loss=0.28 avg=0.88\n",
            "[432 | 1023.75] loss=0.40 avg=0.87\n",
            "[433 | 1026.00] loss=0.37 avg=0.87\n",
            "[434 | 1028.25] loss=0.59 avg=0.86\n",
            "[435 | 1030.49] loss=0.39 avg=0.86\n",
            "[436 | 1032.74] loss=0.27 avg=0.85\n",
            "[437 | 1034.98] loss=0.34 avg=0.85\n",
            "[438 | 1037.23] loss=0.31 avg=0.84\n",
            "[439 | 1039.49] loss=0.26 avg=0.84\n",
            "[440 | 1041.74] loss=0.29 avg=0.83\n",
            "[441 | 1044.00] loss=0.34 avg=0.83\n",
            "[442 | 1046.25] loss=0.29 avg=0.82\n",
            "[443 | 1048.51] loss=0.44 avg=0.82\n",
            "[444 | 1050.75] loss=0.27 avg=0.81\n",
            "[445 | 1053.02] loss=0.29 avg=0.81\n",
            "[446 | 1055.27] loss=0.27 avg=0.80\n",
            "[447 | 1057.53] loss=0.25 avg=0.80\n",
            "[448 | 1059.78] loss=0.32 avg=0.79\n",
            "[449 | 1062.04] loss=0.41 avg=0.79\n",
            "[450 | 1064.29] loss=0.46 avg=0.78\n",
            "[451 | 1066.55] loss=0.40 avg=0.78\n",
            "[452 | 1068.81] loss=0.30 avg=0.78\n",
            "[453 | 1071.06] loss=0.20 avg=0.77\n",
            "[454 | 1073.32] loss=0.40 avg=0.77\n",
            "[455 | 1075.57] loss=0.24 avg=0.76\n",
            "[456 | 1077.84] loss=0.36 avg=0.76\n",
            "[457 | 1080.08] loss=0.20 avg=0.75\n",
            "[458 | 1082.34] loss=0.31 avg=0.75\n",
            "[459 | 1084.60] loss=0.33 avg=0.74\n",
            "[460 | 1086.85] loss=0.35 avg=0.74\n",
            "[461 | 1089.10] loss=0.52 avg=0.74\n",
            "[462 | 1091.36] loss=0.17 avg=0.73\n",
            "[463 | 1093.61] loss=0.30 avg=0.73\n",
            "[464 | 1095.86] loss=0.30 avg=0.72\n",
            "[465 | 1098.11] loss=0.35 avg=0.72\n",
            "[466 | 1100.36] loss=0.25 avg=0.71\n",
            "[467 | 1102.61] loss=0.26 avg=0.71\n",
            "[468 | 1104.86] loss=0.22 avg=0.70\n",
            "[469 | 1107.12] loss=0.37 avg=0.70\n",
            "[470 | 1109.37] loss=0.26 avg=0.70\n",
            "[471 | 1111.62] loss=0.37 avg=0.69\n",
            "[472 | 1113.88] loss=0.29 avg=0.69\n",
            "[473 | 1116.13] loss=0.23 avg=0.68\n",
            "[474 | 1118.38] loss=0.17 avg=0.68\n",
            "[475 | 1120.63] loss=0.20 avg=0.67\n",
            "[476 | 1122.89] loss=0.21 avg=0.67\n",
            "[477 | 1125.14] loss=0.22 avg=0.66\n",
            "[478 | 1127.40] loss=0.21 avg=0.66\n",
            "[479 | 1129.65] loss=0.25 avg=0.66\n",
            "[480 | 1131.90] loss=0.19 avg=0.65\n",
            "[481 | 1134.14] loss=0.21 avg=0.65\n",
            "[482 | 1136.40] loss=0.29 avg=0.64\n",
            "[483 | 1138.66] loss=0.18 avg=0.64\n",
            "[484 | 1140.91] loss=0.27 avg=0.63\n",
            "[485 | 1143.17] loss=0.20 avg=0.63\n",
            "[486 | 1145.42] loss=0.20 avg=0.63\n",
            "[487 | 1147.67] loss=0.20 avg=0.62\n",
            "[488 | 1149.93] loss=0.21 avg=0.62\n",
            "[489 | 1152.19] loss=0.23 avg=0.61\n",
            "[490 | 1154.44] loss=0.23 avg=0.61\n",
            "[491 | 1156.68] loss=0.20 avg=0.61\n",
            "[492 | 1158.93] loss=0.18 avg=0.60\n",
            "[493 | 1161.19] loss=0.18 avg=0.60\n",
            "[494 | 1163.44] loss=0.29 avg=0.59\n",
            "[495 | 1165.69] loss=0.23 avg=0.59\n",
            "[496 | 1167.94] loss=0.15 avg=0.59\n",
            "[497 | 1170.18] loss=0.18 avg=0.58\n",
            "[498 | 1172.44] loss=0.24 avg=0.58\n",
            "[499 | 1174.69] loss=0.18 avg=0.57\n",
            "[500 | 1176.94] loss=0.16 avg=0.57\n",
            "Saving checkpoint/run1/model-500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "bUagiJzBTeoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"Why?\""
      ],
      "metadata": {
        "id": "qzTK7bdIPeOY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess, prefix=prefix, length=150)"
      ],
      "metadata": {
        "id": "ZCaaNXR7kI9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f8e1124-23ba-4221-e28e-7a9adee49728"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why?--But as in the ancient world, men\n",
            "were not instrumentals of God but, rather, are they a state of\n",
            "all-rounding, all-powerful, all-pervading, all-encompassing, all-loving,\n",
            "all-sembling, all-nimble, all-running, with wheels that go around\n",
            "everywhere, absolutely defy all description and all comparison, they\n",
            "were they to attain this happiness or the mean to thresh.\n",
            "\n",
            "20. In all religious systems there is an ever-present danger, whenever\n",
            "a god Countermand decrees an end, an end is thereby attained.\n",
            "\n",
            "21. The recollection of a visit to a memory hole--this is the one\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving model to Google Drive (optional)"
      ],
      "metadata": {
        "id": "zlM6aQYZSccl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QYXmOFl5Bjhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "metadata": {
        "id": "3RUjr4_ZluKi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more texts e.g. on:\n",
        "https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
        "</br></br>\n",
        "You can download them to Colab using code similar to the ones below."
      ],
      "metadata": {
        "id": "OUhaGg_uS6o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/cache/epub/1597/pg1597.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7K9X3K8TEwj",
        "outputId": "d0760c42-a0e4-4dcf-b7cc-ca98aaffa2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 14:49:16--  https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 329071 (321K) [text/plain]\n",
            "Saving to: â€˜pg1597.txtâ€™\n",
            "\n",
            "pg1597.txt          100%[===================>] 321.36K   800KB/s    in 0.4s    \n",
            "\n",
            "2023-03-21 14:49:22 (800 KB/s) - â€˜pg1597.txtâ€™ saved [329071/329071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "metadata": {
        "id": "HYL0wij2m4Gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bf360b-ce90-4a36-d434-44820124b877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 13:25:10--  https://www.gutenberg.org/files/98/98-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 807231 (788K) [text/plain]\n",
            "Saving to: â€˜98-0.txtâ€™\n",
            "\n",
            "98-0.txt            100%[===================>] 788.31K   718KB/s    in 1.1s    \n",
            "\n",
            "2023-02-22 13:25:12 (718 KB/s) - â€˜98-0.txtâ€™ saved [807231/807231]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/matt-dray/tng-stardate/tree/master/data/scripts"
      ],
      "metadata": {
        "id": "VClsbkgRxYvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}